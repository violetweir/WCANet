{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc77f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def create_1d_wavelet_filter(wave, in_size, out_size, type=torch.float):\n",
    "    w = pywt.Wavelet(wave)\n",
    "    dec_hi = torch.tensor(w.dec_hi[::-1], dtype=type)\n",
    "    dec_lo = torch.tensor(w.dec_lo[::-1], dtype=type)\n",
    "    dec_filters = torch.stack([dec_lo, dec_hi], dim=0)\n",
    "\n",
    "    dec_filters = dec_filters[:, None].repeat(in_size, 1, 1)\n",
    "\n",
    "    rec_hi = torch.tensor(w.rec_hi, dtype=type)\n",
    "    rec_lo = torch.tensor(w.rec_lo, dtype=type)\n",
    "    rec_filters = torch.stack([rec_lo, rec_hi], dim=0)\n",
    "\n",
    "    rec_filters = rec_filters[:, None].repeat(out_size, 1, 1)\n",
    "\n",
    "    return dec_filters, rec_filters\n",
    "\n",
    "\n",
    "def create_2d_wavelet_filter(wave, in_size, out_size, type=torch.float):\n",
    "    w = pywt.Wavelet(wave)\n",
    "    dec_hi = torch.tensor(w.dec_hi[::-1], dtype=type)\n",
    "    dec_lo = torch.tensor(w.dec_lo[::-1], dtype=type)\n",
    "    dec_filters = torch.stack([dec_lo.unsqueeze(0) * dec_lo.unsqueeze(1),\n",
    "                               dec_lo.unsqueeze(0) * dec_hi.unsqueeze(1),\n",
    "                               dec_hi.unsqueeze(0) * dec_lo.unsqueeze(1),\n",
    "                               dec_hi.unsqueeze(0) * dec_hi.unsqueeze(1)], dim=0)\n",
    "\n",
    "    dec_filters = dec_filters[:, None].repeat(in_size, 1, 1, 1)\n",
    "\n",
    "    rec_hi = torch.tensor(w.rec_hi, dtype=type)\n",
    "    rec_lo = torch.tensor(w.rec_lo, dtype=type)\n",
    "    rec_filters = torch.stack([rec_lo.unsqueeze(0) * rec_lo.unsqueeze(1),\n",
    "                               rec_lo.unsqueeze(0) * rec_hi.unsqueeze(1),\n",
    "                               rec_hi.unsqueeze(0) * rec_lo.unsqueeze(1),\n",
    "                               rec_hi.unsqueeze(0) * rec_hi.unsqueeze(1)], dim=0)\n",
    "\n",
    "    rec_filters = rec_filters[:, None].repeat(out_size, 1, 1, 1)\n",
    "\n",
    "    return dec_filters, rec_filters\n",
    "\n",
    "\n",
    "def wavelet_1d_transform(x, filters):\n",
    "    b, c, l = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1)\n",
    "    x = F.conv1d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    x = x.reshape(b, c, 2, l // 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inverse_1d_wavelet_transform(x, filters):\n",
    "    b, c, _, l_half = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1)\n",
    "    x = x.reshape(b, c * 2, l_half)\n",
    "    x = F.conv_transpose1d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    return x\n",
    "\n",
    "\n",
    "def wavelet_2d_transform(x, filters):\n",
    "    b, c, h, w = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1, filters.shape[3] // 2 - 1)\n",
    "    x = F.conv2d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    x = x.reshape(b, c, 4, h // 2, w // 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inverse_2d_wavelet_transform(x, filters):\n",
    "    b, c, _, h_half, w_half = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1, filters.shape[3] // 2 - 1)\n",
    "    x = x.reshape(b, c * 4, h_half, w_half)\n",
    "    x = F.conv_transpose2d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38d67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class WTConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, bias=True, wt_levels=1, wt_type='db1'):\n",
    "        super(WTConv2d, self).__init__()\n",
    "\n",
    "        assert in_channels == out_channels\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.wt_levels = wt_levels\n",
    "        self.stride = stride\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.wt_filter, self.iwt_filter = create_2d_wavelet_filter(wt_type, in_channels, in_channels, torch.float)\n",
    "        self.wt_filter = nn.Parameter(self.wt_filter, requires_grad=False)\n",
    "        self.iwt_filter = nn.Parameter(self.iwt_filter, requires_grad=False)\n",
    "\n",
    "        self.base_conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels, bias=bias)\n",
    "        self.base_scale = _ScaleModule([1,in_channels,1,1])\n",
    "\n",
    "        self.wavelet_convs = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels*4, in_channels*4, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels*4, bias=False) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "        self.wavelet_scale = nn.ModuleList(\n",
    "            [_ScaleModule([1,in_channels*4,1,1], init_scale=0.1) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "\n",
    "        if self.stride > 1:\n",
    "            self.do_stride = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.do_stride = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_ll_in_levels = []\n",
    "        x_h_in_levels = []\n",
    "        shapes_in_levels = []\n",
    "\n",
    "        curr_x_ll = x\n",
    "\n",
    "        for i in range(self.wt_levels):\n",
    "            curr_shape = curr_x_ll.shape\n",
    "            shapes_in_levels.append(curr_shape)\n",
    "            if (curr_shape[2] % 2 > 0) or (curr_shape[3] % 2 > 0):\n",
    "                curr_pads = (0, curr_shape[3] % 2, 0, curr_shape[2] % 2)\n",
    "                curr_x_ll = F.pad(curr_x_ll, curr_pads)\n",
    "\n",
    "            curr_x = wavelet_2d_transform(curr_x_ll, self.wt_filter)\n",
    "            curr_x_ll = curr_x[:,:,0,:,:]\n",
    "            \n",
    "            shape_x = curr_x.shape\n",
    "            curr_x_tag = curr_x.reshape(shape_x[0], shape_x[1] * 4, shape_x[3], shape_x[4])\n",
    "            curr_x_tag = self.wavelet_scale[i](self.wavelet_convs[i](curr_x_tag))\n",
    "            curr_x_tag = curr_x_tag.reshape(shape_x)\n",
    "\n",
    "            x_ll_in_levels.append(curr_x_tag[:,:,0,:,:])\n",
    "            x_h_in_levels.append(curr_x_tag[:,:,1:4,:,:])\n",
    "\n",
    "        next_x_ll = 0\n",
    "\n",
    "        for i in range(self.wt_levels-1, -1, -1):\n",
    "            curr_x_ll = x_ll_in_levels.pop()\n",
    "            curr_x_h = x_h_in_levels.pop()\n",
    "            curr_shape = shapes_in_levels.pop()\n",
    "\n",
    "            curr_x_ll = curr_x_ll + next_x_ll\n",
    "\n",
    "            curr_x = torch.cat([curr_x_ll.unsqueeze(2), curr_x_h], dim=2)\n",
    "            next_x_ll = inverse_2d_wavelet_transform(curr_x, self.iwt_filter)\n",
    "\n",
    "            next_x_ll = next_x_ll[:, :, :curr_shape[2], :curr_shape[3]]\n",
    "\n",
    "        x_tag = next_x_ll\n",
    "        assert len(x_ll_in_levels) == 0\n",
    "        \n",
    "        x = self.base_scale(self.base_conv(x))\n",
    "        x = x + x_tag\n",
    "        \n",
    "        if self.do_stride is not None:\n",
    "            x = self.do_stride(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class _ScaleModule(nn.Module):\n",
    "    def __init__(self, dims, init_scale=1.0, init_bias=0):\n",
    "        super(_ScaleModule, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.weight = nn.Parameter(torch.ones(*dims) * init_scale)\n",
    "        self.bias = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mul(self.weight, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb4724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "torch.Size([1, 1024, 3, 3])\n",
      "运算量：464.707M, 参数量：10.862M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# ========== 工具函数（小波滤波器）==========\n",
    "def create_2d_wavelet_filter(wave, in_size, out_size, dtype=torch.float):\n",
    "    w = pywt.Wavelet(wave)\n",
    "    dec_hi = torch.tensor(w.dec_hi[::-1], dtype=dtype)\n",
    "    dec_lo = torch.tensor(w.dec_lo[::-1], dtype=dtype)\n",
    "    dec_filters = torch.stack([\n",
    "        dec_lo.unsqueeze(0) * dec_lo.unsqueeze(1),\n",
    "        dec_lo.unsqueeze(0) * dec_hi.unsqueeze(1),\n",
    "        dec_hi.unsqueeze(0) * dec_lo.unsqueeze(1),\n",
    "        dec_hi.unsqueeze(0) * dec_hi.unsqueeze(1)\n",
    "    ], dim=0)\n",
    "    dec_filters = dec_filters[:, None].repeat(in_size, 1, 1, 1)\n",
    "\n",
    "    rec_hi = torch.tensor(w.rec_hi, dtype=dtype)\n",
    "    rec_lo = torch.tensor(w.rec_lo, dtype=dtype)\n",
    "    rec_filters = torch.stack([\n",
    "        rec_lo.unsqueeze(0) * rec_lo.unsqueeze(1),\n",
    "        rec_lo.unsqueeze(0) * rec_hi.unsqueeze(1),\n",
    "        rec_hi.unsqueeze(0) * rec_lo.unsqueeze(1),\n",
    "        rec_hi.unsqueeze(0) * rec_hi.unsqueeze(1)\n",
    "    ], dim=0)\n",
    "    rec_filters = rec_filters[:, None].repeat(out_size, 1, 1, 1)\n",
    "    return dec_filters, rec_filters\n",
    "\n",
    "\n",
    "def wavelet_2d_transform(x, filters):\n",
    "    b, c, h, w = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1, filters.shape[3] // 2 - 1)\n",
    "    x = F.conv2d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    x = x.reshape(b, c, 4, h // 2, w // 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inverse_2d_wavelet_transform(x, filters):\n",
    "    b, c, _, h_half, w_half = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1, filters.shape[3] // 2 - 1)\n",
    "    x = x.reshape(b, c * 4, h_half, w_half)\n",
    "    x = F.conv_transpose2d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    return x\n",
    "\n",
    "\n",
    "class _ScaleModule(nn.Module):\n",
    "    def __init__(self, dims, init_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(*dims) * init_scale)\n",
    "    def forward(self, x):\n",
    "        return self.weight * x\n",
    "\n",
    "\n",
    "class WTConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, bias=True, wt_levels=1, wt_type='db1'):\n",
    "        super().__init__()\n",
    "        assert in_channels == out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.wt_levels = wt_levels\n",
    "        self.stride = stride\n",
    "\n",
    "        dec_filter, rec_filter = create_2d_wavelet_filter(wt_type, in_channels, in_channels, torch.float)\n",
    "        self.wt_filter = nn.Parameter(dec_filter, requires_grad=False)\n",
    "        self.iwt_filter = nn.Parameter(rec_filter, requires_grad=False)\n",
    "\n",
    "        self.base_conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding='same', \n",
    "                                   groups=in_channels, bias=bias)\n",
    "        self.base_scale = _ScaleModule([1, in_channels, 1, 1])\n",
    "\n",
    "        self.wavelet_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels * 4, in_channels * 4, kernel_size, padding='same',\n",
    "                      groups=in_channels * 4, bias=False) for _ in range(wt_levels)\n",
    "        ])\n",
    "        self.wavelet_scale = nn.ModuleList([\n",
    "            _ScaleModule([1, in_channels * 4, 1, 1], init_scale=0.1) for _ in range(wt_levels)\n",
    "        ])\n",
    "\n",
    "        self.do_stride = nn.AvgPool2d(kernel_size=1, stride=stride) if stride > 1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ll_in_levels = []\n",
    "        x_h_in_levels = []\n",
    "        shapes_in_levels = []\n",
    "        curr_x_ll = x\n",
    "\n",
    "        for i in range(self.wt_levels):\n",
    "            curr_shape = curr_x_ll.shape\n",
    "            shapes_in_levels.append(curr_shape)\n",
    "            if (curr_shape[2] % 2) or (curr_shape[3] % 2):\n",
    "                curr_x_ll = F.pad(curr_x_ll, (0, curr_shape[3] % 2, 0, curr_shape[2] % 2))\n",
    "            curr_x = wavelet_2d_transform(curr_x_ll, self.wt_filter)\n",
    "            curr_x_ll = curr_x[:, :, 0, :, :]\n",
    "            shape_x = curr_x.shape\n",
    "            curr_x_tag = curr_x.reshape(shape_x[0], shape_x[1] * 4, shape_x[3], shape_x[4])\n",
    "            curr_x_tag = self.wavelet_scale[i](self.wavelet_convs[i](curr_x_tag))\n",
    "            curr_x_tag = curr_x_tag.reshape(shape_x)\n",
    "            x_ll_in_levels.append(curr_x_tag[:, :, 0, :, :])\n",
    "            x_h_in_levels.append(curr_x_tag[:, :, 1:4, :, :])\n",
    "\n",
    "        next_x_ll = 0\n",
    "        for i in range(self.wt_levels - 1, -1, -1):\n",
    "            curr_x_ll = x_ll_in_levels.pop() + next_x_ll\n",
    "            curr_x_h = x_h_in_levels.pop()\n",
    "            curr_shape = shapes_in_levels.pop()\n",
    "            curr_x = torch.cat([curr_x_ll.unsqueeze(2), curr_x_h], dim=2)\n",
    "            next_x_ll = inverse_2d_wavelet_transform(curr_x, self.iwt_filter)\n",
    "            next_x_ll = next_x_ll[:, :, :curr_shape[2], :curr_shape[3]]\n",
    "\n",
    "        x_tag = next_x_ll\n",
    "        x = self.base_scale(self.base_conv(x)) + x_tag\n",
    "        if self.do_stride is not None:\n",
    "            x = self.do_stride(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ========== 原有组件（不变）==========\n",
    "def drop_path(x, drop_prob=0., training=False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()\n",
    "    return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob or 0.0\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x): return x\n",
    "\n",
    "class EffectiveSELayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Conv2d(channels, channels, 1)\n",
    "        self.act = nn.Hardsigmoid(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x_se = x.mean([2, 3], keepdim=True)\n",
    "        return x * self.act(self.fc(x_se))\n",
    "\n",
    "class ConvBNLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, filter_size=3, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ch_in, ch_out, filter_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "# ========== 修改后的 Block（含 WTConv2d）==========\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, wt_levels=1, kernel_size=5, drop_path=0., wt_type='db1'):\n",
    "        super().__init__()\n",
    "        self.wt_conv = WTConv2d(dim, dim, kernel_size=kernel_size, wt_levels=wt_levels, wt_type=wt_type)\n",
    "        self.norm = nn.BatchNorm2d(dim)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 2 * dim, 1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(2 * dim, dim, 1)\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "        self.ese = EffectiveSELayer(dim)\n",
    "        self.gamma = nn.Parameter(1e-6 * torch.ones(1, dim, 1, 1), requires_grad=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.wt_conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.ese(x)\n",
    "        x = input + self.drop_path(x * self.gamma)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ========== CSPStage（传递 wt_levels）==========\n",
    "class CSPStage(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, n, stride, p_rates, wt_levels, kernel_size=5, wt_type='db1'):\n",
    "        super().__init__()\n",
    "        ch_mid = (ch_in + ch_out) // 2\n",
    "        self.down = ConvBNLayer(ch_in, ch_mid, 2, stride, 0) if stride == 2 else Identity()\n",
    "        self.conv1 = ConvBNLayer(ch_mid, ch_mid // 2, 1)\n",
    "        self.conv2 = ConvBNLayer(ch_mid, ch_mid // 2, 1)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(ch_mid // 2, wt_levels=wt_levels, kernel_size=kernel_size, drop_path=p_rates[i], wt_type=wt_type)\n",
    "            for i in range(n)\n",
    "        ])\n",
    "        self.attn = EffectiveSELayer(ch_mid)\n",
    "        self.conv3 = ConvBNLayer(ch_mid, ch_out, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        y1 = self.conv1(x)\n",
    "        y2 = self.blocks(self.conv2(x))\n",
    "        y = torch.cat([y1, y2], dim=1)\n",
    "        y = self.attn(y)\n",
    "        return self.conv3(y)\n",
    "\n",
    "\n",
    "# ========== 主干模型 ==========\n",
    "class CSPConvNeXt(nn.Module):\n",
    "    arch_settings = {\n",
    "        'mini':  {'depths': [3,3,9,3], 'dims': [48,96,192,384,768], 'stem': 'va', 'stride': [1,2,2,2]},\n",
    "        'tiny':  {'depths': [3,3,9,3], 'dims': [64,128,256,512,1024], 'stem': 'vb', 'stride': [2,2,2,2]},\n",
    "        'small': {'depths': [3,3,27,3], 'dims': [64,128,256,512,1024], 'stem': 'vb', 'stride': [2,2,2,2]},\n",
    "    }\n",
    "\n",
    "    def __init__(self, arch='tiny', in_chans=3, drop_path_rate=0., class_num=1000,\n",
    "                 kernel_size=5, wt_type='db1', depth_mult=1.0, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        cfg = self.arch_settings[arch]\n",
    "        depths = [int(d * depth_mult) for d in cfg['depths']]\n",
    "        dims   = [int(d * width_mult) for d in cfg['dims']]\n",
    "        stem_type = cfg['stem']\n",
    "        strides   = cfg['stride']\n",
    "\n",
    "        # Stem\n",
    "        if stem_type == 'va':\n",
    "            self.Down_Conv = nn.Sequential(\n",
    "                ConvBNLayer(in_chans, (dims[0]+dims[1])//2, 4, 4, 0)\n",
    "            )\n",
    "        else:\n",
    "            self.Down_Conv = nn.Sequential(\n",
    "                ConvBNLayer(in_chans, dims[0]//2, 2, 2, 0),\n",
    "                ConvBNLayer(dims[0]//2, dims[0]//2, 3, 1, 1),\n",
    "                ConvBNLayer(dims[0]//2, dims[0], 3, 2, 1)\n",
    "            )\n",
    "\n",
    "        # Stages with WT levels: [5,4,3,2]\n",
    "        wt_levels_list = [5, 4, 3, 2]\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        stages = []\n",
    "        for i in range(4):\n",
    "            stages.append(CSPStage(\n",
    "                ch_in=dims[i],\n",
    "                ch_out=dims[i+1],\n",
    "                n=depths[i],\n",
    "                stride=strides[i],\n",
    "                p_rates=dp_rates[sum(depths[:i]):sum(depths[:i+1])],\n",
    "                wt_levels=wt_levels_list[i],\n",
    "                kernel_size=kernel_size,\n",
    "                wt_type=wt_type\n",
    "            ))\n",
    "        self.stages = nn.Sequential(*stages)\n",
    "        self.head = nn.Linear(dims[-1], class_num)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Down_Conv(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        print(x.shape)\n",
    "        x = x.mean([-2, -1])  # global avg pool\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# ========== 快捷构造函数 ==========\n",
    "def e_convnext_mini_wt(**kwargs):\n",
    "    return CSPConvNeXt(arch='mini', **kwargs)\n",
    "\n",
    "def e_convnext_tiny_wt(**kwargs):\n",
    "    return CSPConvNeXt(arch='tiny', **kwargs)\n",
    "\n",
    "def e_convnext_small_wt(**kwargs):\n",
    "    return CSPConvNeXt(arch='small', **kwargs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from thop import profile\n",
    "    from thop import clever_format\n",
    "    # model = StarNet_NEW_CONV()\n",
    "    # x = torch.randn(1, 3, 224, 224).cuda()\n",
    "    # model = model.cuda()  # Move model to GPU\n",
    "    # model.eval()\n",
    "    # y = model(x)\n",
    "    # print(y.shape)\n",
    "    # distillation=False\n",
    "    # pretrained=False\n",
    "    # num_classes=1000\n",
    "    # model = StarNet_NEW_CONV()\n",
    "    # x = torch.randn(1, 3, 224, 224)\n",
    "    # y = model(x)\n",
    "    # print(y.shape)\n",
    "    # print(\"Model and input are on GPU:\", next(model.parameters()).is_cuda)\n",
    "    # model = StarNet_MHSA(dims=[40,80,160,320], depth=[3, 3, 12, 5], learnable_wavelet=True)\n",
    "    model = e_convnext_tiny_wt(class_num=1000)\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")\n",
    "    x = torch.randn(1, 3,224,224).to(\"cuda\")\n",
    "    # y = model(x)\n",
    "    # print(y.shape)\n",
    "\n",
    "    MACs, params = profile(model, inputs=(x,))\n",
    "    # y = model(x)\n",
    "    # print(y.shape)\n",
    "    MACs, params = clever_format([MACs, params], '%.3f')\n",
    "\n",
    "    print(f\"运算量：{MACs}, 参数量：{params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858fc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsanet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
